{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "# torch.set_default_device(device)\n",
    "\n",
    "DATASET = \"./DATASET\"\n",
    "\n",
    "# check if dataset is available\n",
    "if not os.path.exists(DATASET):\n",
    "    print(\"Dataset not found. Please download it from https://www.kaggle.com/datasets/techsash/waste-classification-data and extract it to ./DATASET\")\n",
    "    exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 22564\n",
      "Test dataset: 2513\n",
      "Classes: ['O', 'R']\n"
     ]
    }
   ],
   "source": [
    "# format:\n",
    "# Organic: DATASET/TRAIN/O/...\n",
    "# Recyclable: DATASET/TRAIN/R/...\n",
    "\n",
    "# load dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(DATASET + \"/TRAIN\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder(DATASET + \"/TEST\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# check dataset\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Test dataset: {len(test_dataset)}\")\n",
    "\n",
    "# check classes\n",
    "print(f\"Classes: {train_dataset.classes}\")\n",
    "\n",
    "# Function to extract features and labels from a dataset\n",
    "def extract_features_labels(loader):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for images, targets in loader:\n",
    "        features.append(images.view(images.size(0), -1).numpy())\n",
    "        labels.append(targets.numpy())\n",
    "    features = np.concatenate(features)\n",
    "    labels = np.concatenate(labels)\n",
    "    return features, labels\n",
    "\n",
    "# Extract features and labels\n",
    "train_features, train_labels = extract_features_labels(train_loader)\n",
    "test_features, test_labels = extract_features_labels(test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.7549\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(train_features, train_labels)\n",
    "dt_predictions = dt_clf.predict(test_features)\n",
    "dt_accuracy = accuracy_score(test_labels, dt_predictions)\n",
    "print(f'Decision Tree Accuracy: {dt_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8663\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "rf_clf.fit(train_features, train_labels)\n",
    "rf_predictions = rf_clf.predict(test_features)\n",
    "rf_accuracy = accuracy_score(test_labels, rf_predictions)\n",
    "print(f'Random Forest Accuracy: {rf_accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:16<05:21, 16.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.5186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:33<05:02, 16.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Loss: 0.5003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:51<04:54, 17.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Loss: 0.4933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [01:09<04:40, 17.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Loss: 0.4861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [01:27<04:26, 17.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 0.4771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [01:46<04:12, 18.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Loss: 0.4731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [02:04<03:57, 18.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Loss: 0.4687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [02:22<03:38, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Loss: 0.4630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [02:41<03:20, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Loss: 0.4606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [03:00<03:03, 18.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Loss: 0.4563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [03:17<02:42, 18.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Loss: 0.4539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [03:34<02:23, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Loss: 0.4505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [03:52<02:04, 17.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Loss: 0.4470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [04:09<01:46, 17.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Loss: 0.4423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [04:26<01:27, 17.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Loss: 0.4363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [04:44<01:10, 17.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Loss: 0.4339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [05:02<00:52, 17.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Loss: 0.4300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [05:19<00:35, 17.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Loss: 0.4272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [05:37<00:17, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Loss: 0.4227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [05:55<00:00, 17.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 0.4197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Accuracy: 0.8866\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 9, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(9, 27, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(27, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128) # 64*4*4 comes from the shape of the output of conv3: 64 channels and 4x4 image size\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 classes: Organic and Recyclable\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "# Initialize the network, loss function, and optimizer\n",
    "cnn = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001)\n",
    "\n",
    "# Training the CNN\n",
    "num_epochs = 20\n",
    "for epoch in tqdm.tqdm(range(num_epochs)):\n",
    "    cnn.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Evaluating the CNN\n",
    "cnn.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = cnn(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "cnn_accuracy = correct / total\n",
    "print(f'CNN Accuracy: {cnn_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ben\\Documents\\code\\py\\datamining_examples\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ben\\Documents\\code\\py\\datamining_examples\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# Load a pretrained VGG16 model\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "# Remove the classifier part of the model to get the features\n",
    "vgg16 = nn.Sequential(*list(vgg16.children())[:-1])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "vgg16.eval()\n",
    "\n",
    "# Function to extract features using VGG16\n",
    "def extract_features(loader):\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            output = vgg16(images)\n",
    "            features.append(output.view(images.size(0), -1).numpy())\n",
    "            labels.append(targets.numpy())\n",
    "    features = np.concatenate(features)\n",
    "    labels = np.concatenate(labels)\n",
    "    return features, labels\n",
    "\n",
    "# Extract features and labels\n",
    "train_features, train_labels = extract_features(train_loader)\n",
    "test_features, test_labels = extract_features(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (22564, 25088)\n",
      "Test features shape: (2513, 25088)\n"
     ]
    }
   ],
   "source": [
    "# print features shape\n",
    "print(f\"Train features shape: {train_features.shape}\")\n",
    "print(f\"Test features shape: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy (with feature extraction): 0.7887\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier with VGG16 features\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(train_features, train_labels)\n",
    "dt_predictions = dt_clf.predict(test_features)\n",
    "dt_accuracy = accuracy_score(test_labels, dt_predictions)\n",
    "print(f'Decision Tree Accuracy (with feature extraction): {dt_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy (with feature extraction): 0.8695\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier with VGG16 features\n",
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "rf_clf.fit(train_features, train_labels)\n",
    "rf_predictions = rf_clf.predict(test_features)\n",
    "rf_accuracy = accuracy_score(test_labels, rf_predictions)\n",
    "print(f'Random Forest Accuracy (with feature extraction): {rf_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape after PCA: (22564, 32)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "pca = PCA(n_components=32)  # new dimensionality\n",
    "train_features_pca = pca.fit_transform(train_features)\n",
    "test_features_pca = pca.transform(test_features)\n",
    "\n",
    "print(f\"Train features shape after PCA: {train_features_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy (with PCA features): 0.7581\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier with PCA features\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(train_features_pca, train_labels)\n",
    "dt_predictions = dt_clf.predict(test_features_pca)\n",
    "dt_accuracy = accuracy_score(test_labels, dt_predictions)\n",
    "print(f'Decision Tree Accuracy (with PCA features): {dt_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy (with PCA features): 0.8548\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier with PCA features\n",
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "rf_clf.fit(train_features_pca, train_labels)\n",
    "rf_predictions = rf_clf.predict(test_features_pca)\n",
    "rf_accuracy = accuracy_score(test_labels, rf_predictions)\n",
    "print(f'Random Forest Accuracy (with PCA features): {rf_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape after PCA_128: (22564, 128)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA with 128 components\n",
    "pca_128 = PCA(n_components=128)\n",
    "train_features_pca_128 = pca_128.fit_transform(train_features)\n",
    "test_features_pca_128 = pca_128.transform(test_features)\n",
    "\n",
    "print(f\"Train features shape after PCA_128: {train_features_pca_128.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy (with PCA_128 features): 0.7433\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Classifier with PCA_128 features\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(train_features_pca_128, train_labels)\n",
    "dt_predictions = dt_clf.predict(test_features_pca_128)\n",
    "dt_accuracy = accuracy_score(test_labels, dt_predictions)\n",
    "print(f'Decision Tree Accuracy (with PCA_128 features): {dt_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy (with PCA_128 features): 0.8488\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier with PCA_128 features\n",
    "rf_clf = RandomForestClassifier(n_estimators=100)\n",
    "rf_clf.fit(train_features_pca_128, train_labels)\n",
    "rf_predictions = rf_clf.predict(test_features_pca_128)\n",
    "rf_accuracy = accuracy_score(test_labels, rf_predictions)\n",
    "print(f'Random Forest Accuracy (with PCA_128 features): {rf_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
