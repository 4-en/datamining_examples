{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating   timestamp\n",
      "0       1      110     1.0  1425941529\n",
      "1       1      147     4.5  1425942435\n",
      "2       1      858     5.0  1425941523\n",
      "3       1     1221     5.0  1425941546\n",
      "4       1     1246     5.0  1425941556\n"
     ]
    }
   ],
   "source": [
    "# association rules for movie recommendations\n",
    "\n",
    "MAX_ENTRIES = 100000\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "DATASET = \"DATASET/movies/ratings.csv\"\n",
    "METADATA = \"DATASET/movies/movies_metadata.csv\"\n",
    "LINKS = \"DATASET/movies/links.csv\"\n",
    "\n",
    "links = pd.read_csv(LINKS, low_memory=False, dtype={'imdbId': str, 'movieId': str})\n",
    "metadata = pd.read_csv(METADATA, low_memory=False, dtype={'id': str, 'imdb_id': str})\n",
    "\n",
    "def id_to_imdb_id(id):\n",
    "    \"\"\"Retrieves the IMDb ID of the movie with the given ID from the links DataFrame.\n",
    "\n",
    "    Args:\n",
    "        id (int): The ID of the movie.\n",
    "        links (pd.DataFrame): A pandas DataFrame loaded from the CSV file,\n",
    "                              containing columns like 'movieId' and 'imdbId'.\n",
    "\n",
    "    Returns:\n",
    "        str: The IMDb ID of the movie, or \"Unknown\" if the ID is not found.\n",
    "    \"\"\"\n",
    "\n",
    "    id = str(id)  # Convert to string for comparison\n",
    "\n",
    "    try:\n",
    "        # Efficiently access the IMDb ID using boolean indexing\n",
    "        imdb_id = links[links['movieId'] == id]['imdbId'].values.tolist()[0]\n",
    "        return imdb_id\n",
    "    except (KeyError, IndexError):\n",
    "        # Handle both KeyError (ID not found) and IndexError (empty list)\n",
    "        raise KeyError(id + \" (Unknown)\")\n",
    "\n",
    "def id_to_title(id):\n",
    "    \"\"\"Retrieves the title of the movie with the given ID from the metadata DataFrame.\n",
    "\n",
    "    Args:\n",
    "        id (int): The ID of the movie.\n",
    "        metadata (pd.DataFrame): A pandas DataFrame loaded from the CSV file,\n",
    "                                  containing columns like 'id' and 'title'.\n",
    "\n",
    "    Returns:\n",
    "        str: The title of the movie, or \"Unknown\" if the ID is not found.\n",
    "    \"\"\"\n",
    "\n",
    "    id = str(id)  # Convert to string for comparison\n",
    "    id = id_to_imdb_id(id)  # Convert to IMDb ID for comparison\n",
    "    id = \"tt\" + id  # Prepend \"tt\" to match IMDb format\n",
    "\n",
    "    try:\n",
    "        # Efficiently access the title using boolean indexing\n",
    "        title = metadata[metadata['imdb_id'] == id]['title'].values.tolist()[0]\n",
    "        return title\n",
    "    except (KeyError, IndexError):\n",
    "        # Handle both KeyError (ID not found) and IndexError (empty list)\n",
    "        return id + \" (Unknown)\"\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(DATASET):\n",
    "    print(\"Dataset not found\")\n",
    "    print(\"Please download the dataset from https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset?select=ratings.csv and place it in the DATASET/movies folder\")\n",
    "    exit(1)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(DATASET)\n",
    "\n",
    "\n",
    "# data = data.head(MAX_ENTRIES)\n",
    "\n",
    "print(data.head())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating   timestamp  rating_avg\n",
      "0       1      110     1.0  1425941529    4.016057\n",
      "1       1      858     5.0  1425941523    4.339811\n",
      "2       1     1221     5.0  1425941546    4.263475\n",
      "3       1     1246     5.0  1425941556    3.912803\n",
      "4       1     1968     4.0  1425942148    3.827553\n",
      "Braveheart\n",
      "The Godfather\n",
      "The Godfather: Part II\n",
      "Dead Poets Society\n",
      "The Breakfast Club\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "# normalize rating for each movie:\n",
    "# devide each rating by the average rating for that movie\n",
    "# that way, we don't just get the highest rated movies, but movies that are rated higher than average for a user\n",
    "\n",
    "# then, remove all but the 1000 most rated movies\n",
    "movie_count = data.groupby('movieId')\n",
    "movie_count = movie_count['movieId'].count()\n",
    "movie_count = movie_count.sort_values(ascending=False)\n",
    "\n",
    "first_1000 = movie_count.head(1000)\n",
    "\n",
    "\n",
    "\n",
    "data = data[data['movieId'].isin(first_1000.index)]\n",
    "            \n",
    "\n",
    "movie_avg = data.groupby('movieId')['rating'].mean()\n",
    "\n",
    "data = pd.merge(data, movie_avg, on='movieId', suffixes=('', '_avg'))\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "# print movie names\n",
    "for i in range(5):\n",
    "    print(id_to_title(data['movieId'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260435\n",
      "userId\n",
      "206984      [21, 39, 95, 110, 216, 260, 342, 345, 348, 380]\n",
      "141659    [150, 153, 165, 296, 588, 590, 648, 743, 783, ...\n",
      "153095                                  [527, 3578, 106487]\n",
      "86798     [32, 266, 364, 527, 924, 1080, 1198, 1199, 120...\n",
      "82467           [1, 2, 32, 48, 50, 135, 296, 316, 318, 344]\n",
      "Name: movieId, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# for each user, get all movies that are rated higher than average\n",
    "# this is our input data for the association rule algorithm\n",
    "\n",
    "data = data[data['rating'] > data['rating_avg']]\n",
    "user_movies = data.groupby('userId')['movieId'].apply(list)\n",
    "\n",
    "# max 10 movies per user\n",
    "# first shuffle the list\n",
    "user_movies = user_movies.sample(frac=1)\n",
    "user_movies = user_movies.apply(lambda x: x[:10])\n",
    "\n",
    "print(len(user_movies))\n",
    "print(user_movies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tommy Boy\n",
      "So I Married an Axe Murderer\n",
      "Kingpin\n",
      "Glory\n",
      "Liar Liar\n",
      "The Thomas Crown Affair\n",
      "Almost Famous\n"
     ]
    }
   ],
   "source": [
    "# print movies of user 1\n",
    "for movie in user_movies[66]:\n",
    "    print(id_to_title(movie))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1       2       3       5       6       7       10      11      14      \\\n",
      "0   False   False   False   False   False   False   False   False   False   \n",
      "1   False   False   False   False   False   False   False   False   False   \n",
      "2   False   False   False   False   False   False   False   False   False   \n",
      "3   False   False   False   False   False   False   False   False   False   \n",
      "4    True    True   False   False   False   False   False   False   False   \n",
      "\n",
      "   16      ...  112556  112852  115617  115713  116797  122882  122886  \\\n",
      "0   False  ...   False   False   False   False   False   False   False   \n",
      "1   False  ...   False   False   False   False   False   False   False   \n",
      "2   False  ...   False   False   False   False   False   False   False   \n",
      "3   False  ...   False   False   False   False   False   False   False   \n",
      "4   False  ...   False   False   False   False   False   False   False   \n",
      "\n",
      "   122904  134130  134853  \n",
      "0   False   False   False  \n",
      "1   False   False   False  \n",
      "2   False   False   False  \n",
      "3   False   False   False  \n",
      "4   False   False   False  \n",
      "\n",
      "[5 rows x 1000 columns]\n",
      "Data shape:  (260435, 1000)\n"
     ]
    }
   ],
   "source": [
    "# convert the data to a format that the mlxtend library can use\n",
    "# we need to convert the data to a list of lists, where each list is a list of movies that a user has rated higher than average\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(user_movies).transform(user_movies)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(\"Data shape: \", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455\n",
      "     antecedents consequents  antecedent support  consequent support  \\\n",
      "0           (29)        (32)            0.025918            0.135231   \n",
      "1          (161)       (150)            0.040555            0.122929   \n",
      "2         (1196)       (260)            0.033674            0.113345   \n",
      "3         (1221)       (858)            0.019537            0.061420   \n",
      "4       (32, 34)         (1)            0.022171            0.166414   \n",
      "5        (16, 6)        (32)            0.020408            0.135231   \n",
      "6        (6, 47)        (32)            0.019986            0.135231   \n",
      "7        (6, 47)        (50)            0.019986            0.120249   \n",
      "8     (153, 110)       (150)            0.018888            0.122929   \n",
      "9     (153, 161)       (150)            0.017805            0.122929   \n",
      "10    (153, 165)       (150)            0.020907            0.122929   \n",
      "11   (1210, 260)      (1196)            0.017068            0.033674   \n",
      "12  (1210, 1196)       (260)            0.014199            0.113345   \n",
      "13    (296, 356)       (318)            0.024582            0.156427   \n",
      "14    (356, 527)       (318)            0.016876            0.156427   \n",
      "\n",
      "     support  confidence       lift  leverage  conviction  zhangs_metric  \n",
      "0   0.014933    0.576148   4.260460  0.011428    2.040261       0.785646  \n",
      "1   0.024544    0.605188   4.923075  0.019558    2.221493       0.830558  \n",
      "2   0.021364    0.634436   5.597386  0.017547    2.425441       0.849967  \n",
      "3   0.013835    0.708137  11.529358  0.012635    3.215820       0.931462  \n",
      "4   0.012279    0.553862   3.328221  0.008590    1.868450       0.715400  \n",
      "5   0.011304    0.553904   4.095971  0.008544    1.938526       0.771605  \n",
      "6   0.011938    0.597310   4.416948  0.009235    2.147481       0.789376  \n",
      "7   0.011558    0.578290   4.809113  0.009154    2.086153       0.808214  \n",
      "8   0.010452    0.553365   4.501499  0.008130    1.963729       0.792826  \n",
      "9   0.012264    0.688807   5.603297  0.010075    2.818419       0.836426  \n",
      "10  0.011508    0.550413   4.477491  0.008938    1.950838       0.793245  \n",
      "11  0.010644    0.623622  18.519157  0.010069    2.567434       0.962428  \n",
      "12  0.010644    0.749594   6.613388  0.009034    3.540875       0.861017  \n",
      "13  0.014111    0.574039   3.669701  0.010266    1.980402       0.745832  \n",
      "14  0.010502    0.622298   3.978208  0.007862    2.233436       0.761481  \n"
     ]
    }
   ],
   "source": [
    "# apply the apriori algorithm to find association rules\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "frequent_itemsets = apriori(df, min_support=0.01, use_colnames=True)\n",
    "\n",
    "print(len(frequent_itemsets))\n",
    "\n",
    "# find association rules\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.55)\n",
    "\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      support     itemsets\n",
      "0    0.113345        (260)\n",
      "1    0.095717        (110)\n",
      "2    0.055265         (39)\n",
      "3    0.051057         (21)\n",
      "4    0.028376         (95)\n",
      "..        ...          ...\n",
      "450  0.015601     (1, 141)\n",
      "451  0.010363    (141, 62)\n",
      "452  0.010026   (185, 150)\n",
      "453  0.013835  (858, 1221)\n",
      "454  0.010390   (260, 541)\n",
      "\n",
      "[455 rows x 2 columns]\n",
      "     antecedents consequents  antecedent support  consequent support  \\\n",
      "0     (153, 110)       (150)            0.018888            0.122929   \n",
      "1     (153, 165)       (150)            0.020907            0.122929   \n",
      "2     (356, 527)       (318)            0.016876            0.156427   \n",
      "3     (296, 356)       (318)            0.024582            0.156427   \n",
      "4       (32, 34)         (1)            0.022171            0.166414   \n",
      "5        (6, 47)        (50)            0.019986            0.120249   \n",
      "6        (6, 47)        (32)            0.019986            0.135231   \n",
      "7        (16, 6)        (32)            0.020408            0.135231   \n",
      "8          (161)       (150)            0.040555            0.122929   \n",
      "9     (153, 161)       (150)            0.017805            0.122929   \n",
      "10        (1196)       (260)            0.033674            0.113345   \n",
      "11   (1210, 260)      (1196)            0.017068            0.033674   \n",
      "12  (1210, 1196)       (260)            0.014199            0.113345   \n",
      "13          (29)        (32)            0.025918            0.135231   \n",
      "14        (1221)       (858)            0.019537            0.061420   \n",
      "\n",
      "     support  confidence       lift  leverage  conviction  zhangs_metric  \n",
      "0   0.010452    0.553365   4.501499  0.008130    1.963729       0.792826  \n",
      "1   0.011508    0.550413   4.477491  0.008938    1.950838       0.793245  \n",
      "2   0.010502    0.622298   3.978208  0.007862    2.233436       0.761481  \n",
      "3   0.014111    0.574039   3.669701  0.010266    1.980402       0.745832  \n",
      "4   0.012279    0.553862   3.328221  0.008590    1.868450       0.715400  \n",
      "5   0.011558    0.578290   4.809113  0.009154    2.086153       0.808214  \n",
      "6   0.011938    0.597310   4.416948  0.009235    2.147481       0.789376  \n",
      "7   0.011304    0.553904   4.095971  0.008544    1.938526       0.771605  \n",
      "8   0.024544    0.605188   4.923075  0.019558    2.221493       0.830558  \n",
      "9   0.012264    0.688807   5.603297  0.010075    2.818419       0.836426  \n",
      "10  0.021364    0.634436   5.597386  0.017547    2.425441       0.849967  \n",
      "11  0.010644    0.623622  18.519157  0.010069    2.567434       0.962428  \n",
      "12  0.010644    0.749594   6.613388  0.009034    3.540875       0.861017  \n",
      "13  0.014933    0.576148   4.260460  0.011428    2.040261       0.785646  \n",
      "14  0.013835    0.708137  11.529358  0.012635    3.215820       0.931462  \n"
     ]
    }
   ],
   "source": [
    "# apply fp-growth algorithm to find frequent itemsets\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "\n",
    "frequent_itemsets = fpgrowth(df, min_support=0.01, use_colnames=True)\n",
    "\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# find association rules\n",
    "fp_rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.55)\n",
    "\n",
    "print(fp_rules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apriori rules:\n",
      "Antecedent -> Consequent (support, confidence)\n",
      "['The City of Lost Children']  ->  ['Twelve Monkeys']  ( 0.014932708737304894 ,  0.5761481481481482 )\n",
      "['Crimson Tide']  ->  ['Apollo 13']  ( 0.02454355213392977 ,  0.6051884112857413 )\n",
      "['The Empire Strikes Back']  ->  ['Star Wars']  ( 0.02136425595638067 ,  0.634435575826682 )\n",
      "['The Godfather: Part II']  ->  ['The Godfather']  ( 0.013834546047958224 ,  0.7081367924528302 )\n",
      "['Twelve Monkeys', 'Babe']  ->  ['Toy Story']  ( 0.012279455526330946 ,  0.5538621406304122 )\n",
      "['Casino', 'Heat']  ->  ['Twelve Monkeys']  ( 0.011304164186841246 ,  0.5539040451552211 )\n",
      "['Heat', 'Se7en']  ->  ['Twelve Monkeys']  ( 0.011937719584541248 ,  0.5973102785782901 )\n",
      "['Heat', 'Se7en']  ->  ['The Usual Suspects']  ( 0.011557586345921246 ,  0.5782901056676273 )\n",
      "['Batman Forever', 'Braveheart']  ->  ['Apollo 13']  ( 0.010451744197208516 ,  0.5533645049806871 )\n",
      "['Batman Forever', 'Crimson Tide']  ->  ['Apollo 13']  ( 0.012264096607598826 ,  0.6888074185896054 )\n",
      "['Batman Forever', 'Die Hard: With a Vengeance']  ->  ['Apollo 13']  ( 0.011507669860041854 ,  0.5504132231404959 )\n",
      "['Return of the Jedi', 'Star Wars']  ->  ['The Empire Strikes Back']  ( 0.010643730681360031 ,  0.6236220472440944 )\n",
      "['Return of the Jedi', 'The Empire Strikes Back']  ->  ['Star Wars']  ( 0.010643730681360031 ,  0.7495943753380204 )\n",
      "['Pulp Fiction', 'Forrest Gump']  ->  ['The Shawshank Redemption']  ( 0.014111006585136407 ,  0.5740393626991565 )\n",
      "['Forrest Gump', \"Schindler's List\"]  ->  ['The Shawshank Redemption']  ( 0.01050166068308791 ,  0.6222980659840728 )\n",
      "FP-growth rules:\n",
      "Antecedent -> Consequent (support, confidence)\n",
      "['Batman Forever', 'Braveheart']  ->  ['Apollo 13']  ( 0.010451744197208516 ,  0.5533645049806871 )\n",
      "['Batman Forever', 'Die Hard: With a Vengeance']  ->  ['Apollo 13']  ( 0.011507669860041854 ,  0.5504132231404959 )\n",
      "['Forrest Gump', \"Schindler's List\"]  ->  ['The Shawshank Redemption']  ( 0.01050166068308791 ,  0.6222980659840728 )\n",
      "['Pulp Fiction', 'Forrest Gump']  ->  ['The Shawshank Redemption']  ( 0.014111006585136407 ,  0.5740393626991565 )\n",
      "['Twelve Monkeys', 'Babe']  ->  ['Toy Story']  ( 0.012279455526330946 ,  0.5538621406304122 )\n",
      "['Heat', 'Se7en']  ->  ['The Usual Suspects']  ( 0.011557586345921246 ,  0.5782901056676273 )\n",
      "['Heat', 'Se7en']  ->  ['Twelve Monkeys']  ( 0.011937719584541248 ,  0.5973102785782901 )\n",
      "['Casino', 'Heat']  ->  ['Twelve Monkeys']  ( 0.011304164186841246 ,  0.5539040451552211 )\n",
      "['Crimson Tide']  ->  ['Apollo 13']  ( 0.02454355213392977 ,  0.6051884112857413 )\n",
      "['Batman Forever', 'Crimson Tide']  ->  ['Apollo 13']  ( 0.012264096607598826 ,  0.6888074185896054 )\n",
      "['The Empire Strikes Back']  ->  ['Star Wars']  ( 0.02136425595638067 ,  0.634435575826682 )\n",
      "['Return of the Jedi', 'Star Wars']  ->  ['The Empire Strikes Back']  ( 0.010643730681360031 ,  0.6236220472440944 )\n",
      "['Return of the Jedi', 'The Empire Strikes Back']  ->  ['Star Wars']  ( 0.010643730681360031 ,  0.7495943753380204 )\n",
      "['The City of Lost Children']  ->  ['Twelve Monkeys']  ( 0.014932708737304894 ,  0.5761481481481482 )\n",
      "['The Godfather: Part II']  ->  ['The Godfather']  ( 0.013834546047958224 ,  0.7081367924528302 )\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print the rules in a human-readable format\n",
    "\n",
    "\n",
    "def print_rule(rule):\n",
    "    antecedent = rule['antecedents']\n",
    "    consequent = rule['consequents']\n",
    "    antecedent = [id_to_title(a) for a in antecedent]\n",
    "    consequent = [id_to_title(c) for c in consequent]\n",
    "    print(antecedent, \" -> \", consequent, \" (\", rule['support'], \", \", rule['confidence'], \")\")\n",
    "\n",
    "print(\"Apriori rules:\")\n",
    "print(\"Antecedent -> Consequent (support, confidence)\")\n",
    "for i in range(len(rules)):\n",
    "    print_rule(rules.iloc[i])\n",
    "\n",
    "print(\"FP-growth rules:\")\n",
    "print(\"Antecedent -> Consequent (support, confidence)\")\n",
    "for i in range(len(fp_rules)):\n",
    "    print_rule(fp_rules.iloc[i])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
