{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ben\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# association rules for imdb movie review data\n",
    "import os\n",
    "\n",
    "DATASET = \"DATASET/IMDB Dataset.csv\"\n",
    "\n",
    "if not os.path.exists(DATASET):\n",
    "    print(\"Dataset not found\")\n",
    "    print(\"Download the dataset from https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews and place it in the DATASET folder\")\n",
    "    exit(1)\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "data = pd.read_csv(DATASET)\n",
    "data.head()\n",
    "\n",
    "# print data info\n",
    "print(data.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      " 2   tokens     50000 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "                                              review sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...  positive   \n",
      "1  A wonderful little production. <br /><br />The...  positive   \n",
      "2  I thought this was a wonderful way to spend ti...  positive   \n",
      "3  Basically there's a family where a little boy ...  negative   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
      "\n",
      "                                              tokens  \n",
      "0  [one, reviewers, mentioned, watching, oz, epis...  \n",
      "1  [wonderful, little, production, br, br, filmin...  \n",
      "2  [thought, wonderful, way, spend, time, hot, su...  \n",
      "3  [basically, theres, family, little, boy, jake,...  \n",
      "4  [petter, matteis, love, time, money, visually,...  \n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    # to lower case\n",
    "    text = text.lower()\n",
    "    # remove anything that is not a letter\n",
    "    text = ''.join([w for w in text if w.isalpha() or w == ' '])\n",
    "\n",
    "    # remove stopwords\n",
    "    word_tokens = word_tokenize(text)\n",
    "    text = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    # only take first 10 and last 10 words\n",
    "    # the idea is to keep the most important words. The beginning and end of the review is usually the most important\n",
    "    # and contain general information about the movie. Specific details are usually in the middle,\n",
    "    # and will not be useful for our analysis\n",
    "\n",
    "    # remove the words movie, film, etc\n",
    "    text = [w for w in text if w not in ['movie', 'film', 'movies', 'films']]\n",
    "\n",
    "    if len(text) > 20:\n",
    "        text = text[:10] + text[-10:]\n",
    "\n",
    "    return text\n",
    "\n",
    "data['tokens'] = data['review'].apply(clean_text)\n",
    "\n",
    "# print data info\n",
    "print(data.info())\n",
    "\n",
    "# print data\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['one', 'reviewers', 'mentioned', 'watching', 'oz', 'episode', 'youll', 'hooked', 'right', 'exactly', 'oz', 'may', 'become', 'comfortable', 'uncomfortable', 'viewingthats', 'get', 'touch', 'darker', 'side']\n"
     ]
    }
   ],
   "source": [
    "review_tokens = data['tokens'].tolist()\n",
    "\n",
    "# for testing, only use first 1000 reviews\n",
    "review_tokens = review_tokens[:1000]\n",
    "\n",
    "print(len(review_tokens))\n",
    "print(review_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aaargh  aamir     ab  abbott  abhorrent  ability  abilitybr   able  \\\n",
      "0   False  False  False   False      False    False      False  False   \n",
      "1   False  False  False   False      False    False      False  False   \n",
      "2   False  False  False   False      False    False      False  False   \n",
      "3   False  False  False   False      False    False      False  False   \n",
      "4   False  False  False   False      False    False      False  False   \n",
      "\n",
      "   abominable  abomination  ...   zidi  zingers  zodiac  zombie  \\\n",
      "0       False        False  ...  False    False   False   False   \n",
      "1       False        False  ...  False    False   False   False   \n",
      "2       False        False  ...  False    False   False   False   \n",
      "3       False        False  ...  False    False   False    True   \n",
      "4       False        False  ...  False    False   False   False   \n",
      "\n",
      "   zombierelated  zombies  zoology     zp  zucker  zzzzzzzzzzzzzzzzzz  \n",
      "0          False    False    False  False   False               False  \n",
      "1          False    False    False  False   False               False  \n",
      "2          False    False    False  False   False               False  \n",
      "3          False    False    False  False   False               False  \n",
      "4          False    False    False  False   False               False  \n",
      "\n",
      "[5 rows x 6413 columns]\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(review_tokens).transform(review_tokens)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     support             itemsets\n",
      "0      0.023         (absolutely)\n",
      "1      0.011             (across)\n",
      "2      0.027             (acting)\n",
      "3      0.022             (action)\n",
      "4      0.018             (actors)\n",
      "..       ...                  ...\n",
      "499    0.010    (watch, watching)\n",
      "500    0.010      (ever, one, br)\n",
      "501    0.010    (ever, one, seen)\n",
      "502    0.012   (ever, one, worst)\n",
      "503    0.014  (ever, worst, seen)\n",
      "\n",
      "[504 rows x 2 columns]\n",
      "      antecedents consequents  antecedent support  consequent support  \\\n",
      "0       (believe)      (cant)               0.022               0.034   \n",
      "1          (read)      (book)               0.027               0.032   \n",
      "2          (else)        (br)               0.019               0.268   \n",
      "3         (waste)      (dont)               0.028               0.078   \n",
      "4          (ever)       (one)               0.059               0.200   \n",
      "5          (ever)     (worst)               0.059               0.042   \n",
      "6         (worst)      (ever)               0.042               0.059   \n",
      "7      (favorite)       (one)               0.019               0.200   \n",
      "8        (pretty)      (good)               0.033               0.117   \n",
      "9        (havent)      (seen)               0.016               0.074   \n",
      "10          (ive)       (one)               0.035               0.200   \n",
      "11          (ive)      (seen)               0.035               0.074   \n",
      "12         (must)       (see)               0.034               0.120   \n",
      "13        (worst)       (one)               0.042               0.200   \n",
      "14        (worst)      (seen)               0.042               0.074   \n",
      "15        (waste)      (time)               0.028               0.077   \n",
      "16     (ever, br)       (one)               0.017               0.200   \n",
      "17   (ever, seen)       (one)               0.022               0.200   \n",
      "18    (ever, one)     (worst)               0.025               0.042   \n",
      "19  (ever, worst)       (one)               0.024               0.200   \n",
      "20   (worst, one)      (ever)               0.021               0.059   \n",
      "21  (ever, worst)      (seen)               0.024               0.074   \n",
      "22   (ever, seen)     (worst)               0.022               0.042   \n",
      "23  (worst, seen)      (ever)               0.017               0.059   \n",
      "\n",
      "    support  confidence       lift  leverage  conviction  zhangs_metric  \n",
      "0     0.010    0.454545  13.368984  0.009252    1.771000       0.946012  \n",
      "1     0.011    0.407407  12.731481  0.010136    1.633500       0.947024  \n",
      "2     0.012    0.631579   2.356638  0.006908    1.986857       0.586816  \n",
      "3     0.012    0.428571   5.494505  0.009816    1.613500       0.841564  \n",
      "4     0.025    0.423729   2.118644  0.013200    1.388235       0.561105  \n",
      "5     0.024    0.406780   9.685230  0.021522    1.614914       0.952976  \n",
      "6     0.024    0.571429   9.685230  0.021522    2.195667       0.936065  \n",
      "7     0.011    0.578947   2.894737  0.007200    1.900000       0.667223  \n",
      "8     0.014    0.424242   3.626004  0.010139    1.533632       0.748929  \n",
      "9     0.010    0.625000   8.445946  0.008816    2.469333       0.895935  \n",
      "10    0.015    0.428571   2.142857  0.008000    1.400000       0.552677  \n",
      "11    0.016    0.457143   6.177606  0.013410    1.705789       0.868523  \n",
      "12    0.014    0.411765   3.431373  0.009920    1.496000       0.733511  \n",
      "13    0.021    0.500000   2.500000  0.012600    1.600000       0.626305  \n",
      "14    0.017    0.404762   5.469755  0.013892    1.555680       0.853003  \n",
      "15    0.017    0.607143   7.884972  0.014844    2.349455       0.898330  \n",
      "16    0.010    0.588235   2.941176  0.006600    1.942857       0.671414  \n",
      "17    0.010    0.454545   2.272727  0.005600    1.466667       0.572597  \n",
      "18    0.012    0.480000  11.428571  0.010950    1.842308       0.935897  \n",
      "19    0.012    0.500000   2.500000  0.007200    1.600000       0.614754  \n",
      "20    0.012    0.571429   9.685230  0.010761    2.195667       0.915986  \n",
      "21    0.014    0.583333   7.882883  0.012224    2.222400       0.894614  \n",
      "22    0.014    0.636364  15.151515  0.013076    2.634500       0.955010  \n",
      "23    0.014    0.823529  13.958126  0.012997    5.332333       0.944412  \n"
     ]
    }
   ],
   "source": [
    "# apriori to find frequent itemsets\n",
    "frequent_itemsets = apriori(df, min_support=0.01, use_colnames=True)\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.4)\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     support        itemsets\n",
      "0      0.200           (one)\n",
      "1      0.061           (get)\n",
      "2      0.053      (watching)\n",
      "3      0.026           (may)\n",
      "4      0.015         (right)\n",
      "..       ...             ...\n",
      "499    0.012    (funny, one)\n",
      "500    0.011    (book, read)\n",
      "501    0.010   (one, horror)\n",
      "502    0.012      (br, else)\n",
      "503    0.010  (havent, seen)\n",
      "\n",
      "[504 rows x 2 columns]\n",
      "      antecedents consequents  antecedent support  consequent support  \\\n",
      "0      (favorite)       (one)               0.019               0.200   \n",
      "1          (must)       (see)               0.034               0.120   \n",
      "2       (believe)      (cant)               0.022               0.034   \n",
      "3           (ive)      (seen)               0.035               0.074   \n",
      "4           (ive)       (one)               0.035               0.200   \n",
      "5         (worst)       (one)               0.042               0.200   \n",
      "6         (worst)      (seen)               0.042               0.074   \n",
      "7          (ever)     (worst)               0.059               0.042   \n",
      "8         (worst)      (ever)               0.042               0.059   \n",
      "9     (ever, one)     (worst)               0.025               0.042   \n",
      "10  (ever, worst)       (one)               0.024               0.200   \n",
      "11   (worst, one)      (ever)               0.021               0.059   \n",
      "12  (ever, worst)      (seen)               0.024               0.074   \n",
      "13   (ever, seen)     (worst)               0.022               0.042   \n",
      "14  (worst, seen)      (ever)               0.017               0.059   \n",
      "15         (ever)       (one)               0.059               0.200   \n",
      "16     (ever, br)       (one)               0.017               0.200   \n",
      "17   (ever, seen)       (one)               0.022               0.200   \n",
      "18       (pretty)      (good)               0.033               0.117   \n",
      "19        (waste)      (time)               0.028               0.077   \n",
      "20        (waste)      (dont)               0.028               0.078   \n",
      "21         (read)      (book)               0.027               0.032   \n",
      "22         (else)        (br)               0.019               0.268   \n",
      "23       (havent)      (seen)               0.016               0.074   \n",
      "\n",
      "    support  confidence       lift  leverage  conviction  zhangs_metric  \n",
      "0     0.011    0.578947   2.894737  0.007200    1.900000       0.667223  \n",
      "1     0.014    0.411765   3.431373  0.009920    1.496000       0.733511  \n",
      "2     0.010    0.454545  13.368984  0.009252    1.771000       0.946012  \n",
      "3     0.016    0.457143   6.177606  0.013410    1.705789       0.868523  \n",
      "4     0.015    0.428571   2.142857  0.008000    1.400000       0.552677  \n",
      "5     0.021    0.500000   2.500000  0.012600    1.600000       0.626305  \n",
      "6     0.017    0.404762   5.469755  0.013892    1.555680       0.853003  \n",
      "7     0.024    0.406780   9.685230  0.021522    1.614914       0.952976  \n",
      "8     0.024    0.571429   9.685230  0.021522    2.195667       0.936065  \n",
      "9     0.012    0.480000  11.428571  0.010950    1.842308       0.935897  \n",
      "10    0.012    0.500000   2.500000  0.007200    1.600000       0.614754  \n",
      "11    0.012    0.571429   9.685230  0.010761    2.195667       0.915986  \n",
      "12    0.014    0.583333   7.882883  0.012224    2.222400       0.894614  \n",
      "13    0.014    0.636364  15.151515  0.013076    2.634500       0.955010  \n",
      "14    0.014    0.823529  13.958126  0.012997    5.332333       0.944412  \n",
      "15    0.025    0.423729   2.118644  0.013200    1.388235       0.561105  \n",
      "16    0.010    0.588235   2.941176  0.006600    1.942857       0.671414  \n",
      "17    0.010    0.454545   2.272727  0.005600    1.466667       0.572597  \n",
      "18    0.014    0.424242   3.626004  0.010139    1.533632       0.748929  \n",
      "19    0.017    0.607143   7.884972  0.014844    2.349455       0.898330  \n",
      "20    0.012    0.428571   5.494505  0.009816    1.613500       0.841564  \n",
      "21    0.011    0.407407  12.731481  0.010136    1.633500       0.947024  \n",
      "22    0.012    0.631579   2.356638  0.006908    1.986857       0.586816  \n",
      "23    0.010    0.625000   8.445946  0.008816    2.469333       0.895935  \n"
     ]
    }
   ],
   "source": [
    "# fp growth\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "\n",
    "frequent_itemsets_fp = fpgrowth(df, min_support=0.01, use_colnames=True)\n",
    "print(frequent_itemsets_fp)\n",
    "\n",
    "rules_fp = association_rules(frequent_itemsets_fp, metric=\"confidence\", min_threshold=0.4)\n",
    "print(rules_fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
